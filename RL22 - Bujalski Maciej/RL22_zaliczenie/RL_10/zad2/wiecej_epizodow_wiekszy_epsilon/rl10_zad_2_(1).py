# -*- coding: utf-8 -*-
"""RL10_zad_2 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ojui3ytdg3yOOe3PJQ6YI_9Ohkq5NYKg
"""

import numpy as np
import gym
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers import Dense
from collections import deque
import random
import tensorflow as tf

env = gym.make("CartPole-v1")
state = env.reset()

model = Sequential()
model.add(Dense(units = 50, input_dim=4, activation='relu'))
model.add(Dense(units = 50, activation = "relu"))
model.add(Dense(units = 2, activation = "linear"))

opt = tf.keras.optimizers.Adam(learning_rate=0.001)
#opt = tf.keras.optimizers.SGD(learning_rate=0.001)

model.compile(loss='MSE',optimizer=opt)
model.summary()

"""------------------------------------"""

train_episodes = 200
epsilon = 0.3
gamma = 0.99
max_steps = 200
state = env.reset()

Loss = []
Rewards = []

for e in range(1, train_episodes+1):
  epsilon = epsilon - (1/train_episodes)
  total_reward = 0
  t = 0

  state = env.reset()
  state = np.reshape(state, [1, 4])  
  
  done = False
  while t < max_steps and done == False:

    Qs = model.predict(state)[0]

    if np.random.rand()<epsilon:
      action = env.action_space.sample()
    else:
      action = np.argmax(Qs)

    next_state, reward, done, _ = env.step(action)
    next_state = np.reshape(next_state, [1, 4])

    total_reward += reward

   
    if done:
      y = reward
    else:
      y = reward + gamma*np.max(model.predict(next_state)[0])
    
    Q_target = model.predict(state)
    Q_target[0][action] = y
          
    h = model.fit(state,Q_target,epochs=1,verbose=0)

    loss = h.history['loss'][0]
    
    state = next_state
    t+=1
  
  print(e," R=",total_reward," L=",loss)
  Rewards.append(total_reward)
  Loss.append(loss)

plt.subplot(211)
plt.ylabel('Suma nagród')  
plt.title('Suma nagród w epizodzie')
plt.plot(list(range(train_episodes)),Rewards,"b")

plt.subplot(212)
plt.xlabel('epizod')
plt.ylabel('błąd')  
plt.title('Loss per epoch')
plt.plot(list(range(train_episodes)),Loss,"r")

plt.show()